{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefc2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.0;num_classes:2\n",
      "---final upsample expand_first---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/monika/miniconda3/envs/miccaienv/lib/python3.9/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.001 beta 0.7 Average training loss is  tensor(5.5231) and average DICE score is tensor(0.6103) and average cc loss is tensor(0.5216, grad_fn=<DivBackward0>)\n",
      "[ 0.10 mins] After 3 epochs, the Average validations loss is  tensor(0.5767) and average DICE score is tensor(0.6053)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.001, 'beta', 0.7)\n",
      "[ 0.54 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.001 beta 0.9 Average training loss is  tensor(5.5694) and average DICE score is tensor(0.6073) and average cc loss is tensor(0.5248, grad_fn=<DivBackward0>)\n",
      "[ 0.14 mins] After 3 epochs, the Average validations loss is  tensor(0.5820) and average DICE score is tensor(0.6028)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.001, 'beta', 0.7)\n",
      "[ 0.61 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.001 beta 0.95 Average training loss is  tensor(5.4544) and average DICE score is tensor(0.6107) and average cc loss is tensor(0.5154, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5622) and average DICE score is tensor(0.6059)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.001, 'beta', 0.95)\n",
      "[ 0.62 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.01 beta 0.7 Average training loss is  tensor(5.4064) and average DICE score is tensor(0.6120) and average cc loss is tensor(0.5108, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5574) and average DICE score is tensor(0.6058)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.01, 'beta', 0.7)\n",
      "[ 0.59 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.01 beta 0.9 Average training loss is  tensor(5.3506) and average DICE score is tensor(0.6120) and average cc loss is tensor(0.5051, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5592) and average DICE score is tensor(0.6057)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.01, 'beta', 0.7)\n",
      "[ 0.60 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 0.01 beta 0.95 Average training loss is  tensor(5.3436) and average DICE score is tensor(0.6118) and average cc loss is tensor(0.5044, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5552) and average DICE score is tensor(0.6057)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 0.01, 'beta', 0.95)\n",
      "[ 0.57 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 1 beta 0.7 Average training loss is  tensor(5.3496) and average DICE score is tensor(0.6117) and average cc loss is tensor(0.5049, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5516) and average DICE score is tensor(0.6058)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.62 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 1 beta 0.9 Average training loss is  tensor(5.3467) and average DICE score is tensor(0.6122) and average cc loss is tensor(0.5048, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5562) and average DICE score is tensor(0.6058)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.62 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 2 alpha 1 beta 0.95 Average training loss is  tensor(5.3328) and average DICE score is tensor(0.6119) and average cc loss is tensor(0.5032, grad_fn=<DivBackward0>)\n",
      "[ 0.14 mins] After 3 epochs, the Average validations loss is  tensor(0.5553) and average DICE score is tensor(0.6057)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.62 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 4 alpha 0.001 beta 0.7 Average training loss is  tensor(5.3353) and average DICE score is tensor(0.6117) and average cc loss is tensor(0.5034, grad_fn=<DivBackward0>)\n",
      "[ 0.12 mins] After 3 epochs, the Average validations loss is  tensor(0.5559) and average DICE score is tensor(0.6056)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.60 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 4 alpha 0.001 beta 0.9 Average training loss is  tensor(5.3723) and average DICE score is tensor(0.6122) and average cc loss is tensor(0.5073, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5551) and average DICE score is tensor(0.6059)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.60 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 4 alpha 0.001 beta 0.95 Average training loss is  tensor(5.3324) and average DICE score is tensor(0.6119) and average cc loss is tensor(0.5031, grad_fn=<DivBackward0>)\n",
      "[ 0.13 mins] After 3 epochs, the Average validations loss is  tensor(0.5630) and average DICE score is tensor(0.6055)\n",
      "best hparams: ('max_epochs', 2, 'n', 3, 'lamda', 0.4, 4, 'batch_size, ', 'num_workers', 2, 'alpha', 1, 'beta', 0.7)\n",
      "[ 0.71 mins] After 3 epochs with n 3 lamda 0.4 4 batch_size,  num_workers 4 alpha 0.01 beta 0.7 Average training loss is  tensor(5.3639) and average DICE score is tensor(0.6110) and average cc loss is tensor(0.5058, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import voxelmorph2d as vm2d\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import os\n",
    "from skimage.transform import resize\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.lines import Line2D\n",
    "use_gpu = torch.cuda.is_available()\n",
    "import sys\n",
    "\n",
    "\n",
    "class VoxelMorph():\n",
    "    \"\"\"\n",
    "    VoxelMorph Class is a higher level interface for both 2D and 3D\n",
    "    Voxelmorph classes. It makes training easier and is scalable.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dims, use_gpu=False):\n",
    "        updated_lr = 0.001\n",
    "        self.dims = input_dims\n",
    "        self.vm = vm2d\n",
    "        self.voxelmorph = vm2d.VoxelMorph2d(input_dims[0] * 2, use_gpu)\n",
    "        self.optimizer = optim.Adam(self.voxelmorph.parameters(), lr=updated_lr, weight_decay=0, amsgrad=True)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        #self.optimizer = optim.SGD(\n",
    "        #    self.voxelmorph.parameters(), lr=1e-4, momentum=0.99)\n",
    "        \n",
    "        self.params = {'batch_size': 3,\n",
    "                       'shuffle': True,\n",
    "                       'num_workers': 6,\n",
    "                       'worker_init_fn': np.random.seed(42)\n",
    "                       }\n",
    "        \n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "    def check_dims(self, x):\n",
    "        try:\n",
    "            if x.shape[1:] == self.dims:\n",
    "                return\n",
    "            else:\n",
    "                raise TypeError\n",
    "        except TypeError as e:\n",
    "            print(\"Invalid Dimension Error. The supposed dimension is \",\n",
    "                  self.dims, \"But the dimension of the input is \", x.shape[1:])\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.check_dims(x)\n",
    "        return voxelmorph(x)\n",
    "\n",
    "#     def calculate_loss(self, y, ytrue, n=7, lamda=10, is_training=True):\n",
    "#         loss = self.criterion(y,ytrue) + self.vm.vox_morph_loss(y, ytrue, n, lamda)\n",
    "#         #loss = self.vm.vox_morph_loss(y, ytrue, n, lamda)\n",
    "#         return loss, self.vm.vox_morph_loss(y, ytrue, n, lamda)\n",
    "    \n",
    "    def calculate_loss(self, y, ytrue, alpha, beta, n=9, lamda=0.01, is_training=True):\n",
    "        loss = alpha * self.criterion(y,ytrue) + beta * self.vm.vox_morph_loss(y, ytrue, n, lamda)\n",
    "        return loss, self.vm.vox_morph_loss(y, ytrue, n, lamda)\n",
    "\n",
    "    def train_model(self, batch_moving, batch_fixed, lr = 0.001, n=7, lamda=10, return_metric_score=True):\n",
    "        updated_lr = round(lr * np.power(1 - (0) / 5,0.9),8)\n",
    "        self.optimizer.zero_grad()\n",
    "        batch_fixed, batch_moving = batch_fixed.to(\n",
    "            self.device), batch_moving.to(self.device)\n",
    "        registered_image = self.voxelmorph(batch_moving, batch_fixed)\n",
    "        train_loss, cc_sm_loss = self.calculate_loss(\n",
    "            registered_image, batch_fixed, n, lamda)\n",
    "        train_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        if return_metric_score:\n",
    "            train_dice_score = self.vm.dice_score(\n",
    "                registered_image, batch_fixed)\n",
    "            return train_loss, train_dice_score, updated_lr, cc_sm_loss\n",
    "        return train_loss, updated_lr\n",
    "\n",
    "    def get_test_loss(self, batch_moving, batch_fixed, n=7, lamda=10):\n",
    "        with torch.set_grad_enabled(False):\n",
    "            registered_image = self.voxelmorph(batch_moving, batch_fixed)\n",
    "            val_loss = self.vm.vox_morph_loss(\n",
    "                registered_image, batch_fixed, n, lamda)\n",
    "            val_dice_score = self.vm.dice_score(registered_image, batch_fixed)\n",
    "            return val_loss, val_dice_score\n",
    "\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for converting the data into batches.\n",
    "    The data.Dataset class is a pyTorch class which help\n",
    "    in speeding up  this process with effective parallelization\n",
    "    \"\"\"\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "\n",
    "    def __init__(self, list_IDs):\n",
    "        'Initialization'\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        fixed_image = torch.Tensor(\n",
    "            resize(io.imread('NIH/test_xrays/' + 'a_' + ID), (224, 224, 3)))\n",
    "        moving_image = torch.Tensor(\n",
    "            resize(io.imread('NIH/test_xrays/' + 'b_' + ID), (224, 224, 3)))\n",
    "        return fixed_image, moving_image\n",
    "\n",
    "    \n",
    "ns = [3, 7, 10, 12]\n",
    "lamdas = [0.4, 3, 5, 10]\n",
    "batch_sizes = [4, 6, 15, 20, 25]\n",
    "num_workerss = [2, 4, 6]\n",
    "max_epochs = [30, 50]\n",
    "alphas = [0.1, 0.5, 0.9]\n",
    "betas = [0.1, 0.5, 0.9, 0.95]\n",
    "\n",
    "\n",
    "def main():\n",
    "    vm = VoxelMorph(\n",
    "        (3, 224, 224))  # Object of the higher level class\n",
    "    DATA_PATH = 'NIH/xrays/'\n",
    "    params = {'batch_size': 3,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 6,\n",
    "              'worker_init_fn': np.random.seed(42)\n",
    "              }\n",
    "\n",
    "    # max_epochs = ...\n",
    "    filename = list(set([x.split('_')[1] + '_' + x.split('_')[2]\n",
    "                         for x in os.listdir('NIH/test_xrays/')]))\n",
    "    partition = {}\n",
    "    partition['train'], partition['validation'] = train_test_split(\n",
    "        filename, test_size=0.33, random_state=42)\n",
    "    \n",
    "    # Generators\n",
    "    training_set = Dataset(partition['train'])\n",
    "    training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = Dataset(partition['validation'])\n",
    "    validation_generator = data.DataLoader(validation_set, **params)\n",
    "    \n",
    "    updated_lr = 0.001\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    best_hparams = None\n",
    "    best_loss = sys.maxsize\n",
    "    # Loop over epochs\n",
    "\n",
    "    \n",
    "    for n in ns:\n",
    "        for lamda in lamdas:\n",
    "            for batch_size in batch_sizes:\n",
    "                for num_workers in num_workerss:\n",
    "                    for alpha in alphas:\n",
    "                        for beta in betas:\n",
    "                            for epoch in max_epochs:\n",
    "                                start_time = time.time()\n",
    "                                train_loss = 0\n",
    "                                train_dice_score = 0\n",
    "                                val_loss = 0\n",
    "                                val_dice_score = 0\n",
    "                                cc_sm_loss = 0\n",
    "                                for batch_fixed, batch_moving in training_generator:\n",
    "                                    loss, dice, updated_lr, cc_sm = vm.train_model(batch_moving, batch_fixed, updated_lr)\n",
    "                                    train_dice_score += dice.data\n",
    "                                    train_loss += loss.data\n",
    "                                    cc_sm_loss += cc_sm\n",
    "                                print('[', \"{0:.2f}\".format((time.time() - start_time) / 60), 'mins]', 'After', epoch + 1, \n",
    "                                      'epochs with', 'n', n, 'lamda', lamda, batch_size, 'batch_size, ', 'num_workers', \n",
    "                                      num_workers, 'alpha', alpha, 'beta', beta, 'Average training loss is ', train_loss *\n",
    "                                      params['batch_size'] / len(training_set), 'and average DICE score is', \n",
    "                                      train_dice_score.data * params['batch_size'] / len(training_set), \n",
    "                                      'and average cc loss is', cc_sm_loss * params['batch_size'] / len(training_set))\n",
    "                                # Testing time\n",
    "                                start_time = time.time()\n",
    "                                for batch_fixed, batch_moving in validation_generator:\n",
    "                                    # Transfer to GPU\n",
    "                                    loss, dice = vm.get_test_loss(batch_moving, batch_fixed)\n",
    "                                    val_dice_score += dice.data\n",
    "                                    val_loss += loss.data\n",
    "\n",
    "\n",
    "                                if best_hparams is None or val_loss < best_loss:\n",
    "                                    #best_hparams = (n, lamda, batch_size, num_workers, max_epochs, alpha, beta)\n",
    "                                    best_hparams = ('max_epochs', epoch, 'n', n, 'lamda', lamda, batch_size, 'batch_size, ', \n",
    "                                    'num_workers', num_workers, 'alpha', alpha, 'beta', beta)\n",
    "                                    best_loss = val_loss    \n",
    "\n",
    "\n",
    "\n",
    "                                print('[', \"{0:.2f}\".format((time.time() - start_time) / 60), 'mins]', 'After', epoch + 1, 'epochs, the Average validations loss is ', val_loss *\n",
    "                                      params['batch_size'] / len(validation_set), 'and average DICE score is', val_dice_score.data * params['batch_size'] / len(validation_set))\n",
    "                                print(f\"best hparams: {best_hparams}\")                \n",
    "                        \n",
    "                        \n",
    "                                             \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a52d116",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miccaienv",
   "language": "python",
   "name": "miccaienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
